Deep learning has achieved great success in various real-world applications. As deep neural networks are getting deeper and wider, the inference and training cost of deep neural networks increases signiﬁcantly. Since one round of inference or one iteration in the training phase of a deep neural network is typically modeled as a computation graph, existing works propose to optimize computation graph by performing a sequence of functional equivalent graph substitutions, leading to higher inference and training eﬃciency. In this work, we formally deﬁne the Optimizing Computation Graph using Graph Substitutions (OCGGS) problem, and provide its hardness results theoretically. We develop two exact and eﬃcient methods to the OCGGS problem. The pruning-based algorithm eliminates the examination of redundant graph substitution sequences, and the dynamic programming with pruning algorithm makes use of the explored graph substitution sequences. To further speed up the search process, we propose a sampling heuristic which is eﬀective to optimize complex computation graphs with polynomial time and space complexity. Extensive experiments on various DNN architectures are conducted to verify the eﬀectiveness and eﬃciency of our proposed solutions compared with the existing techniques.