Machine learning algorithms are commonly specified in linear algebra (LA). LA expressions can be rewritten into more efficient forms, by taking advantage of input properties such as sparsity, as well as program properties such as common subexpressions and fusible  operators.  The complex interaction among these propertiesâ€™ impact on the execution cost poses a challenge to optimizing compilers.  Existing compilers resort to intricate heuristics that complicate the codebase and add maintenance  cost,  but  fail  to  search  through  the large space of equivalent LA expressions to find the cheapest  one.   We  introduce  a  general  optimization  technique for LA expressions,  by converting the LA expressions into Relational Algebra (RA) expressions, optimizing the latter, then  converting  the  result  back  to  (optimized)  LA  expressions. The rewrite rules we design in this approach are complete, meaning that any equivalent LA expression is covered in the search space. The challenge is the major size of the search space, and we address this by adopting and extending a technique used in compilers, called equality saturation. Our optimizer, SPORES, uses rule sampling to quickly cover vast portions of the search space; it then uses a constraint solver to extract the optimal plan from the covered space, or alternatively uses a greedy algorithm to shorten compile time. We integrate SPORES into SystemML and validate it empirically across a spectrum of machine learning tasks; SPORES can derive all existing hand-coded optimizations in SystemML, and perform new optimizations that lead to up to 10X speedup.